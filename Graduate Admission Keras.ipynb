{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Admission_Predict.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         400 non-null    int64  \n",
      " 1   GRE Score          400 non-null    int64  \n",
      " 2   TOEFL Score        400 non-null    int64  \n",
      " 3   University Rating  400 non-null    int64  \n",
      " 4   SOP                400 non-null    float64\n",
      " 5   LOR                400 non-null    float64\n",
      " 6   CGPA               400 non-null    float64\n",
      " 7   Research           400 non-null    int64  \n",
      " 8   Chance of Admit    400 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 28.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Serial No.',axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0          337          118                  4  4.5   4.5  9.65         1   \n",
       "1          324          107                  4  4.0   4.5  8.87         1   \n",
       "2          316          104                  3  3.0   3.5  8.00         1   \n",
       "3          322          110                  3  3.5   2.5  8.67         1   \n",
       "4          314          103                  2  2.0   3.0  8.21         0   \n",
       "..         ...          ...                ...  ...   ...   ...       ...   \n",
       "395        324          110                  3  3.5   3.5  9.04         1   \n",
       "396        325          107                  3  3.0   3.5  9.11         1   \n",
       "397        330          116                  4  5.0   4.5  9.45         1   \n",
       "398        312          103                  3  3.5   4.0  8.78         0   \n",
       "399        333          117                  4  5.0   4.0  9.66         1   \n",
       "\n",
       "     Chance of Admit   \n",
       "0                0.92  \n",
       "1                0.76  \n",
       "2                0.72  \n",
       "3                0.80  \n",
       "4                0.65  \n",
       "..                ...  \n",
       "395              0.82  \n",
       "396              0.84  \n",
       "397              0.91  \n",
       "398              0.67  \n",
       "399              0.95  \n",
       "\n",
       "[400 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:-1].values\n",
    "y = df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.76210664,  1.74697064,  0.79882862, ...,  1.16732114,\n",
       "         1.76481828,  0.90911166],\n",
       "       [ 0.62765641, -0.06763531,  0.79882862, ...,  1.16732114,\n",
       "         0.45515126,  0.90911166],\n",
       "       [-0.07046681, -0.56252785, -0.07660001, ...,  0.05293342,\n",
       "        -1.00563118,  0.90911166],\n",
       "       ...,\n",
       "       [ 1.15124883,  1.41704229,  0.79882862, ...,  1.16732114,\n",
       "         1.42900622,  0.90911166],\n",
       "       [-0.41952842, -0.72749202, -0.07660001, ...,  0.61012728,\n",
       "         0.30403584, -1.09997489],\n",
       "       [ 1.41304503,  1.58200646,  0.79882862, ...,  0.61012728,\n",
       "         1.78160888,  0.90911166]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train ,X_test ,y_train ,y_test = train_test_split(X ,y ,test_size=0.1 ,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(21, activation='relu' ,input_dim = X_train.shape[1]))\n",
    "model.add(Dense(1 ,activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 21)                168       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 22        \n",
      "=================================================================\n",
      "Total params: 190\n",
      "Trainable params: 190\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.7708 - val_loss: 0.5131\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.4042 - val_loss: 0.2645\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.2276 - val_loss: 0.1520\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1467 - val_loss: 0.1132\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.1110 - val_loss: 0.0921\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0896 - val_loss: 0.0774\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0724 - val_loss: 0.0620\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0588 - val_loss: 0.0518\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0482 - val_loss: 0.0423\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0394 - val_loss: 0.0352\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0328 - val_loss: 0.0294\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0277 - val_loss: 0.0249\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0239 - val_loss: 0.0221\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0209 - val_loss: 0.0188\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0184 - val_loss: 0.0169\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0165 - val_loss: 0.0151\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0148 - val_loss: 0.0141\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.0131\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0121\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.0113\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0106 - val_loss: 0.0108\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.0103\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0087 - val_loss: 0.0095\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 0.0090\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.0088\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0076 - val_loss: 0.0085\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0084\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.0081\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0067 - val_loss: 0.0081\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.0079\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0063 - val_loss: 0.0078\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0061 - val_loss: 0.0076\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.0075\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0059 - val_loss: 0.0075\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.0074\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0056 - val_loss: 0.0073\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0074\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0073\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.0072\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0053 - val_loss: 0.0072\n",
      "Epoch 42/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0052 - val_loss: 0.0071\n",
      "Epoch 43/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0071\n",
      "Epoch 44/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.0070\n",
      "Epoch 45/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0050 - val_loss: 0.0072\n",
      "Epoch 46/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0070\n",
      "Epoch 47/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0070\n",
      "Epoch 48/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0070\n",
      "Epoch 49/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.0069\n",
      "Epoch 50/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.0069\n",
      "Epoch 51/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0070\n",
      "Epoch 52/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0069\n",
      "Epoch 53/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0069\n",
      "Epoch 54/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0069\n",
      "Epoch 55/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.0068\n",
      "Epoch 56/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.0070\n",
      "Epoch 57/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.0070\n",
      "Epoch 58/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.0068\n",
      "Epoch 59/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.0068\n",
      "Epoch 60/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.0068\n",
      "Epoch 61/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.0068\n",
      "Epoch 62/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.0067\n",
      "Epoch 63/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.0067\n",
      "Epoch 64/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.0069\n",
      "Epoch 65/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0067\n",
      "Epoch 66/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0066\n",
      "Epoch 67/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.0066\n",
      "Epoch 68/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0066\n",
      "Epoch 69/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0040 - val_loss: 0.0067\n",
      "Epoch 70/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0041 - val_loss: 0.0065\n",
      "Epoch 71/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.0065\n",
      "Epoch 72/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.0065\n",
      "Epoch 73/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.0064\n",
      "Epoch 74/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.0066\n",
      "Epoch 75/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.0064\n",
      "Epoch 76/100\n",
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.0064\n",
      "Epoch 77/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0038 - val_loss: 0.0064\n",
      "Epoch 78/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0039 - val_loss: 0.0065\n",
      "Epoch 79/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0038 - val_loss: 0.0063\n",
      "Epoch 80/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0038 - val_loss: 0.0066\n",
      "Epoch 81/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0037 - val_loss: 0.0065\n",
      "Epoch 82/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0037 - val_loss: 0.0063\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.0065\n",
      "Epoch 84/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0037 - val_loss: 0.0063\n",
      "Epoch 85/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.0062\n",
      "Epoch 86/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0036 - val_loss: 0.0063\n",
      "Epoch 87/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.0061\n",
      "Epoch 88/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.0061\n",
      "Epoch 89/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.0061\n",
      "Epoch 90/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0035 - val_loss: 0.0061\n",
      "Epoch 91/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0034 - val_loss: 0.0061\n",
      "Epoch 92/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0034 - val_loss: 0.0060\n",
      "Epoch 93/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0034 - val_loss: 0.0060\n",
      "Epoch 94/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0034 - val_loss: 0.0060\n",
      "Epoch 95/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0034 - val_loss: 0.0059\n",
      "Epoch 96/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0034 - val_loss: 0.0059\n",
      "Epoch 97/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0033 - val_loss: 0.0059\n",
      "Epoch 98/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0033 - val_loss: 0.0059\n",
      "Epoch 99/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0033 - val_loss: 0.0058\n",
      "Epoch 100/100\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0033 - val_loss: 0.0058\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs=100,batch_size=10,verbose=1 ,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000027A03A6C790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6639044699041284"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.7708313465118408,\n",
       "  0.404163658618927,\n",
       "  0.2275962084531784,\n",
       "  0.1467362344264984,\n",
       "  0.11104034632444382,\n",
       "  0.08963770419359207,\n",
       "  0.07236098498106003,\n",
       "  0.05879543349146843,\n",
       "  0.048193641006946564,\n",
       "  0.03941158950328827,\n",
       "  0.03276287764310837,\n",
       "  0.027660241350531578,\n",
       "  0.023933524265885353,\n",
       "  0.020852698013186455,\n",
       "  0.01843002252280712,\n",
       "  0.016512248665094376,\n",
       "  0.014826703816652298,\n",
       "  0.013508401811122894,\n",
       "  0.012375814840197563,\n",
       "  0.011421994306147099,\n",
       "  0.010617573745548725,\n",
       "  0.009869844652712345,\n",
       "  0.009260128252208233,\n",
       "  0.00872123334556818,\n",
       "  0.008288171142339706,\n",
       "  0.007874641567468643,\n",
       "  0.007558569312095642,\n",
       "  0.007178265135735273,\n",
       "  0.006948268972337246,\n",
       "  0.006718447431921959,\n",
       "  0.00651996023952961,\n",
       "  0.006313251331448555,\n",
       "  0.006099794525653124,\n",
       "  0.006037463899701834,\n",
       "  0.0058874343521893024,\n",
       "  0.0057395282201468945,\n",
       "  0.0056322235614061356,\n",
       "  0.005549797788262367,\n",
       "  0.005456911865621805,\n",
       "  0.005376064218580723,\n",
       "  0.005346786696463823,\n",
       "  0.005223055835813284,\n",
       "  0.005137999542057514,\n",
       "  0.005095838103443384,\n",
       "  0.005009043496102095,\n",
       "  0.004938975442200899,\n",
       "  0.004872046876698732,\n",
       "  0.004857731517404318,\n",
       "  0.0048526497557759285,\n",
       "  0.004727434832602739,\n",
       "  0.004628216847777367,\n",
       "  0.004673975985497236,\n",
       "  0.004613119643181562,\n",
       "  0.004569705575704575,\n",
       "  0.004497256595641375,\n",
       "  0.004454982001334429,\n",
       "  0.0044393520802259445,\n",
       "  0.0044552031904459,\n",
       "  0.004350258037447929,\n",
       "  0.00431064423173666,\n",
       "  0.0042747752740979195,\n",
       "  0.004198859911412001,\n",
       "  0.004220770671963692,\n",
       "  0.004204106517136097,\n",
       "  0.004117355681955814,\n",
       "  0.004064672626554966,\n",
       "  0.00409381790086627,\n",
       "  0.004034440964460373,\n",
       "  0.003973561339080334,\n",
       "  0.004052282776683569,\n",
       "  0.003949992824345827,\n",
       "  0.003948802128434181,\n",
       "  0.003911767620593309,\n",
       "  0.0038700199220329523,\n",
       "  0.0038641197606921196,\n",
       "  0.0038602331187576056,\n",
       "  0.0037791309878230095,\n",
       "  0.003860238939523697,\n",
       "  0.0037847794592380524,\n",
       "  0.003774221520870924,\n",
       "  0.0037079984322190285,\n",
       "  0.003706336487084627,\n",
       "  0.0036617890000343323,\n",
       "  0.0036744512617588043,\n",
       "  0.0035574333742260933,\n",
       "  0.003617667593061924,\n",
       "  0.0035292627289891243,\n",
       "  0.003529330948367715,\n",
       "  0.0035125191789120436,\n",
       "  0.0035055552143603563,\n",
       "  0.0034464187920093536,\n",
       "  0.003396016778424382,\n",
       "  0.0034014438278973103,\n",
       "  0.003389005782082677,\n",
       "  0.0033821023534983397,\n",
       "  0.00337167177349329,\n",
       "  0.0033414699137210846,\n",
       "  0.003348427126184106,\n",
       "  0.0033300714567303658,\n",
       "  0.003287366358563304],\n",
       " 'val_loss': [0.5130655765533447,\n",
       "  0.2644681930541992,\n",
       "  0.15200971066951752,\n",
       "  0.11320117861032486,\n",
       "  0.09206174314022064,\n",
       "  0.07742129266262054,\n",
       "  0.06199214234948158,\n",
       "  0.05175943672657013,\n",
       "  0.04231199249625206,\n",
       "  0.03520336002111435,\n",
       "  0.029449710622429848,\n",
       "  0.024912962689995766,\n",
       "  0.02206607535481453,\n",
       "  0.018786922097206116,\n",
       "  0.016890497878193855,\n",
       "  0.015142621472477913,\n",
       "  0.014100690372288227,\n",
       "  0.013076146133244038,\n",
       "  0.012121850624680519,\n",
       "  0.011263457126915455,\n",
       "  0.010775336064398289,\n",
       "  0.010287673212587833,\n",
       "  0.009707091376185417,\n",
       "  0.00949113629758358,\n",
       "  0.008974738419055939,\n",
       "  0.008758234791457653,\n",
       "  0.00852126907557249,\n",
       "  0.008365152403712273,\n",
       "  0.008149373345077038,\n",
       "  0.008122729137539864,\n",
       "  0.007886051200330257,\n",
       "  0.007780862506479025,\n",
       "  0.007646464742720127,\n",
       "  0.007539671380072832,\n",
       "  0.0075480397790670395,\n",
       "  0.007408973295241594,\n",
       "  0.007312493398785591,\n",
       "  0.007358615752309561,\n",
       "  0.00733904866501689,\n",
       "  0.007227334659546614,\n",
       "  0.007209476549178362,\n",
       "  0.00711402203887701,\n",
       "  0.007057089824229479,\n",
       "  0.007026882842183113,\n",
       "  0.007170080207288265,\n",
       "  0.006999786477535963,\n",
       "  0.00703844940289855,\n",
       "  0.007041009608656168,\n",
       "  0.006934703327715397,\n",
       "  0.006945822853595018,\n",
       "  0.007027528248727322,\n",
       "  0.006910241208970547,\n",
       "  0.006898930761963129,\n",
       "  0.006885905750095844,\n",
       "  0.006845060270279646,\n",
       "  0.006964555010199547,\n",
       "  0.006976213771849871,\n",
       "  0.0068290336057543755,\n",
       "  0.0068396213464438915,\n",
       "  0.006813759915530682,\n",
       "  0.006805010139942169,\n",
       "  0.006710035726428032,\n",
       "  0.00669075595214963,\n",
       "  0.006893731653690338,\n",
       "  0.006660774350166321,\n",
       "  0.00663880817592144,\n",
       "  0.006605260539799929,\n",
       "  0.006592872552573681,\n",
       "  0.0067177521996200085,\n",
       "  0.00654510036110878,\n",
       "  0.0065217106603085995,\n",
       "  0.006469924468547106,\n",
       "  0.006432069931179285,\n",
       "  0.0065957335755229,\n",
       "  0.006403622683137655,\n",
       "  0.0063904873095452785,\n",
       "  0.006399298086762428,\n",
       "  0.006489370949566364,\n",
       "  0.006315324455499649,\n",
       "  0.006626609247177839,\n",
       "  0.006535808090120554,\n",
       "  0.0062753381207585335,\n",
       "  0.006472354289144278,\n",
       "  0.006282907910645008,\n",
       "  0.006213074550032616,\n",
       "  0.006279845256358385,\n",
       "  0.006105484906584024,\n",
       "  0.0060965134762227535,\n",
       "  0.0061275530606508255,\n",
       "  0.0060705202631652355,\n",
       "  0.006058565340936184,\n",
       "  0.006031714845448732,\n",
       "  0.005997929722070694,\n",
       "  0.005958930589258671,\n",
       "  0.005947412922978401,\n",
       "  0.005942484829574823,\n",
       "  0.00592091353610158,\n",
       "  0.005874644964933395,\n",
       "  0.005835669115185738,\n",
       "  0.00581308314576745]}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27a0251a1f0>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
